{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNR4h9rVO9oVqTn3xYwj0sx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DartDoesData/build-within-ai-overview/blob/main/Using_Nominatim_and_OpenAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading from a Database and Preparing for Address Correction**\n",
        "\n",
        "In this section, you'll learn how to connect to an SQLite database, query data into a pandas DataFrame, and prepare for address correction using Nominatim (an open-source geocoding tool). This is a foundational skill in working with AI tools that often require structured data.\n",
        "\n",
        "---\n",
        "\n",
        "## **What We'll Do**\n",
        "1. Download an SQLite database file from GitHub.\n",
        "2. Connect to the database using Python.\n",
        "3. Query data and load it into a pandas DataFrame for easy manipulation.\n",
        "4. Prepare the data for address correction with Nominatim.\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "1. **Import Libraries**:\n",
        "   - `sqlite3`: For connecting to SQLite databases.\n",
        "   - `pandas`: For working with data in table-like structures.\n",
        "   - `requests`: To download the database file from a URL.\n",
        "\n",
        "2. **Download the Database File**:\n",
        "   - The database is hosted on GitHub, and we use `requests` to fetch it.\n",
        "   - The file is saved locally as `students-demo.db`.\n",
        "   - Note that for this is for demonstration purposes. Best practice is to leverage a hosted database for live interactions.\n",
        "\n",
        "3. **Connect to the Database**:\n",
        "   - Use `sqlite3.connect` to establish a connection to the database.\n",
        "\n",
        "4. **Query the Data**:\n",
        "   - Write an SQL query (`SELECT * FROM students`) to retrieve all records from the `students` table.\n",
        "   - Load the query results into a pandas DataFrame for further processing.\n",
        "\n",
        "5. **Close the Connection**:\n",
        "   - Always close the database connection after querying to free up resources.\n",
        "\n",
        "6. **Display the Data**:\n",
        "   - Use `students_df.head()` to preview the first few rows of the data."
      ],
      "metadata": {
        "id": "-T99UE8ggZxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3SFaTFprLWw"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import libraries\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "db_url = \"https://raw.githubusercontent.com/DartDoesData/build-within-ai-overview/main/db/students-demo.db\"\n",
        "db_file = \"students-demo.db\"\n",
        "\n",
        "try:\n",
        "    # Step 2: Download the SQLite DB file from GitHub\n",
        "    response = requests.get(db_url)\n",
        "    response.raise_for_status()\n",
        "    with open(db_file, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error downloading database: {e}\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    # Step 3: Connect to the SQLite Database\n",
        "    conn = sqlite3.connect(db_file)\n",
        "\n",
        "    # Step 4: Query the database and load into Pandas DataFrame\n",
        "    query = \"SELECT * FROM students\"\n",
        "    students_df = pd.read_sql_query(query, conn)\n",
        "\n",
        "finally:\n",
        "    # Step 5: Close the connection\n",
        "    if conn:\n",
        "        conn.close()\n",
        "\n",
        "# Step 6: Display the DataFrame\n",
        "display(students_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validating and Enriching Addresses with Nominatim**\n",
        "\n",
        "Now that we have the data, our next task is to correct and validate addresses using **Nominatim**, a free geocoding tool. This tool converts address information into detailed components such as latitude, longitude, city, state, and postal code.\n",
        "\n",
        "In this section, you'll learn how to use Nominatim to validate and enrich addresses. This builds on the data we loaded from the SQLite database and prepares it for use in AI-powered applications.\n",
        "\n",
        "---\n",
        "\n",
        "## **What We'll Do**\n",
        "1. Install and import necessary libraries.\n",
        "2. Set up the Nominatim geolocator.\n",
        "3. Define a function to process addresses and retrieve detailed components.\n",
        "4. Handle errors gracefully to ensure consistent output.\n",
        "5. Apply the function to the dataset and display the updated DataFrame.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step-by-Step Explanation**\n",
        "1. **Install and Import Libraries**:\n",
        "   - Install the `geopy` library, which provides geocoding capabilities.\n",
        "   - Import `geopy` tools, `pandas`, and any necessary error-handling modules.\n",
        "\n",
        "2. **Set Up the Nominatim Geolocator**:\n",
        "   - Create a Nominatim geolocator instance with a unique user agent string.\n",
        "   - This step prepares the geolocator to process address inputs.\n",
        "\n",
        "3. **Define the Address Parsing Function**:\n",
        "   - Use `geolocator.geocode` to find geographic coordinates (latitude, longitude) for a given address.\n",
        "   - Use `geolocator.reverse` to translate coordinates into detailed address components.\n",
        "   - If no data is found, return a default template with empty values.\n",
        "\n",
        "4. **Apply the Function to the Dataset**:\n",
        "   - Use the `parse_address` function to process each address in the dataset.\n",
        "   - Expand the parsed details into new columns in the DataFrame.\n",
        "\n",
        "5. **Preview the Updated DataFrame**:\n",
        "   - Display the enriched data, including both the original address and the newly added details."
      ],
      "metadata": {
        "id": "NT-69jCfh3s0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the geopy library for geocoding capabilities\n",
        "!pip install geopy --quiet\n",
        "\n",
        "import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "# Step 2: Set Up the Nominatim Geolocator\n",
        "geolocator = Nominatim(user_agent=\"build_within\")\n",
        "\n",
        "# Step 3: Define the Address Parsing Function\n",
        "# Return a default template with empty values if parsing fails\n",
        "EMPTY_RESULT = {\n",
        "    'street_number': None,\n",
        "    'street_name': None,\n",
        "    'city': None,\n",
        "    'state': None,\n",
        "    'zip': None,\n",
        "    'lat': None,\n",
        "    'lng': None,\n",
        "    'display_name': None\n",
        "}\n",
        "\n",
        "def parse_address(address):\n",
        "    try:\n",
        "        # Retrieve geographic coordinates for the address\n",
        "        location = geolocator.geocode(address, timeout=10)\n",
        "        if location:\n",
        "            # Retrieve detailed address components based on the coordinates\n",
        "            details = geolocator.reverse(\n",
        "                (location.latitude, location.longitude),\n",
        "                timeout=10,\n",
        "                addressdetails=True\n",
        "            ).raw.get('address', {})\n",
        "            return {\n",
        "                'street_number': details.get('house_number'),\n",
        "                'street_name': details.get('road'),\n",
        "                'city': details.get('city', details.get('town', details.get('village'))),\n",
        "                'state': details.get('state', details.get('region')),\n",
        "                'zip': details.get('postcode'),\n",
        "                'lat': location.latitude,\n",
        "                'lng': location.longitude,\n",
        "                'display_name': location.address\n",
        "            }\n",
        "    except GeocoderTimedOut:\n",
        "        # Handle timeout errors and return default empty result\n",
        "        pass\n",
        "\n",
        "    # Return the default empty result for failures\n",
        "    return EMPTY_RESULT\n",
        "\n",
        "# Step 5: Apply the Function to the Dataset\n",
        "# Apply the parse_address function to each row in the DataFrame\n",
        "# Expand the parsed details into separate columns\n",
        "students_df[['street_number', 'street_name', 'city', 'state', 'zip', 'lat', 'lng', 'display_name']] = \\\n",
        "    students_df['full_address'].apply(lambda x: pd.Series(parse_address(x)))\n",
        "\n",
        "# Step 6: Preview the Updated DataFrame\n",
        "display(students_df.head())\n"
      ],
      "metadata": {
        "id": "EVRB4kFkrtf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generating AI-Powered Narratives with OpenAI**\n",
        "\n",
        "In this step, we will use OpenAI's GPT model to generate narratives for each record in our dataset. This process demonstrates how to integrate generative AI into a real-world data enrichment workflow.\n",
        "\n",
        "---\n",
        "\n",
        "## **What We'll Do**\n",
        "1. Import necessary libraries and retrieve the API key.\n",
        "2. Define key parameters for interacting with the OpenAI API.\n",
        "3. Write a function to generate AI responses based on custom prompts.\n",
        "4. Iterate through the dataset and update it with AI-generated narratives.\n",
        "5. Display the dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **How the code works**\n",
        "\n",
        "1. **Import Necessary Libraries**:\n",
        "   - Import `os`, `userdata`, and `requests` for API interaction and key management.\n",
        "\n",
        "2. **Retrieve the OpenAI API Key**:\n",
        "   - Use `userdata.get` to retrieve the API key from Colab Secrets.\n",
        "   - If the key is missing, raise an exception with instructions for adding it.\n",
        "\n",
        "3. **Define Constants and Parameters**:\n",
        "   - Set the maximum token limit for the LLM response (`MAX_TOKENS`).\n",
        "   - Configure verbosity, mood, and tone parameters for the assistant's behavior.\n",
        "   - Define the API endpoint and required headers.\n",
        "\n",
        "4. **Write the Function to Generate AI Responses**:\n",
        "   - Send a prompt to the OpenAI API via a POST request.\n",
        "   - Parse the JSON response to extract the generated content.\n",
        "   - Handle errors gracefully by raising descriptive exceptions.\n",
        "\n",
        "5. **Iterate Through the Dataset**:\n",
        "   - Loop through each row of the `students_df` DataFrame.\n",
        "   - For each row:\n",
        "     - Generate a narrative about the city and state using the `generate_response` function.\n",
        "     - Generate a narrative about the first name using a separate prompt.\n",
        "   - Update the DataFrame with the generated narratives.\n",
        "\n",
        "6. **Display the Updated Dataset**:\n",
        "   - Show the dataset with new columns for `address_narrative` and `name_narrative`."
      ],
      "metadata": {
        "id": "2GVmKHtpsGnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import necessary libraries\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import requests\n",
        "\n",
        "# Step 2: Retrieve the OpenAI API key\n",
        "# Attempt to retrieve the API key from Colab Secrets\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    print('API key retrieved from Colab Secrets.')\n",
        "else:\n",
        "    raise Exception('API key not found in Colab Secrets. Please add it under \"Secrets\".')\n",
        "\n",
        "# Step 3: Define constants and parameters\n",
        "# Maximum tokens for the LLM response\n",
        "MAX_TOKENS = 1024\n",
        "\n",
        "# Configurable parameters for LLM behavior\n",
        "verbosity = 'brief'\n",
        "mood = 'optimistic'\n",
        "tone = 'direct'\n",
        "\n",
        "# OpenAI API endpoint and headers\n",
        "openai_endpoint = 'https://api.openai.com/v1/chat/completions'\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {OPENAI_API_KEY}',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "# Step 4: Function to generate LLM response\n",
        "# This function sends a prompt to the OpenAI API and returns the response content\n",
        "def generate_response(prompt):\n",
        "    request_payload = {\n",
        "        'model': 'gpt-4o',\n",
        "        'messages': [\n",
        "            {'role': 'system', 'content': f'You are an assistant with {verbosity} verbosity. Your mood is {mood}, and you speak with a {tone} tone.'},\n",
        "            {'role': 'user', 'content': prompt}\n",
        "        ],\n",
        "        'max_tokens': MAX_TOKENS\n",
        "    }\n",
        "\n",
        "    response = requests.post(openai_endpoint, headers=headers, json=request_payload)\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        return response_json['choices'][0]['message']['content'].strip()\n",
        "    else:\n",
        "        raise Exception(f\"Error: {response.status_code} - {response.text}\")\n",
        "\n",
        "# Step 5: Iterate through the DataFrame and update narratives\n",
        "# Loop through each row in the DataFrame to generate AI-generated narratives\n",
        "for idx, row in students_df.iterrows():\n",
        "    try:\n",
        "        # Generate narrative for city and state\n",
        "        address_prompt = f\"What are three popular things to do in {row['city']}, {row['state']}?\"\n",
        "        address_fact = generate_response(address_prompt)\n",
        "        students_df.loc[idx, 'address_narrative'] = address_fact\n",
        "\n",
        "        # Generate narrative for first name\n",
        "        name_prompt = f\"Tell a brief, fun fact about the name {row['first_name']}.\"\n",
        "        name_fact = generate_response(name_prompt)\n",
        "        students_df.loc[idx, 'name_narrative'] = name_fact\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log any errors during the iteration\n",
        "        print(f\"Error processing row {idx}: {e}\")\n",
        "\n",
        "# Step 6: Display the updated DataFrame\n",
        "# Show the DataFrame with new AI-generated narratives\n",
        "display(students_df)"
      ],
      "metadata": {
        "id": "MehVsMf40k-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working with LLM-Generated Code in a Parsed Format**\n",
        "\n",
        "When working with code or responses generated by a Language Model (LLM), it's often helpful to parse the responses into structured data. This approach ensures consistency and usability, particularly when dealing with hierarchical or tabular information like recipes.\n",
        "\n",
        "---\n",
        "\n",
        "## **What We'll Do**\n",
        "1. Define dataclasses to organize structured data.\n",
        "2. Create functions to parse JSON responses into these dataclasses.\n",
        "3. Configure an OpenAI API request to retrieve structured recipe data.\n",
        "4. Parse the API response into a structured format.\n",
        "5. Convert the structured data into pandas DataFrames for further analysis.\n",
        "6. Display the results for easy interpretation.\n",
        "\n",
        "---\n",
        "\n",
        "### **How the code works**\n",
        "\n",
        "1. **Define Dataclasses**:\n",
        "   - `Ingredient`, `Step`, `CookingMethod`, and `Recipe` dataclasses represent the hierarchy of recipe components.\n",
        "   - These classes make it easier to parse and manipulate structured JSON data.\n",
        "\n",
        "2. **Create Parsing Functions**:\n",
        "   - `parse_ingredients`: Converts a list of ingredient dictionaries into `Ingredient` objects.\n",
        "   - `parse_steps`: Converts a list of step dictionaries into `Step` objects.\n",
        "   - `parse_cooking_modes`: Converts a list of cooking method dictionaries into `CookingMethod` objects, each containing its steps.\n",
        "\n",
        "3. **Configure the OpenAI API Request**:\n",
        "   - Define the API endpoint, headers, and request payload.\n",
        "   - Request the recipe in a structured JSON format.\n",
        "\n",
        "4. **Send the API Request and Handle the Response**:\n",
        "   - Use `requests.post` to send the payload to the OpenAI API.\n",
        "   - Parse the response into JSON, handling edge cases like extra text outside the JSON structure.\n",
        "\n",
        "5. **Parse the Response into Structured Data**:\n",
        "   - Use the dataclasses and parsing functions to organize the response into `Recipe`, `Ingredient`, and `CookingMethod` objects.\n",
        "\n",
        "6. **Convert Structured Data into DataFrames**:\n",
        "   - Create a pandas DataFrame for ingredients, showing the name, quantity, and unit.\n",
        "   - Create another DataFrame for cooking methods, including the method, step number, and step description.\n",
        "\n",
        "7. **Display the Results**:\n",
        "   - Print and display the DataFrames to review the structured recipe details."
      ],
      "metadata": {
        "id": "jU95KrWBskEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Define dataclasses for structured responses\n",
        "# These classes represent the recipe structure for clear parsing and organization\n",
        "@dataclass\n",
        "class Ingredient:\n",
        "    name: str\n",
        "    quantity: str\n",
        "    unit: str\n",
        "\n",
        "@dataclass\n",
        "class Step:\n",
        "    step: int\n",
        "    description: str\n",
        "\n",
        "@dataclass\n",
        "class CookingMethod:\n",
        "    method: str  # e.g., \"microwave\", \"bake\", \"fry\", etc.\n",
        "    steps: List[Step]  # List of steps specific to this method\n",
        "\n",
        "@dataclass\n",
        "class Recipe:\n",
        "    title: str\n",
        "    ingredients: List[Ingredient]  # List of Ingredient objects\n",
        "    cooking_modes: List[CookingMethod]  # List of CookingMethod objects\n",
        "\n",
        "# Step 2: Functions to parse JSON response\n",
        "# These functions convert raw JSON into structured dataclass objects\n",
        "def parse_ingredients(ingredient_list):\n",
        "    return [Ingredient(**item) for item in ingredient_list]\n",
        "\n",
        "def parse_steps(steps_list):\n",
        "    return [Step(**step) for step in steps_list]\n",
        "\n",
        "def parse_cooking_modes(modes_list):\n",
        "    methods = []\n",
        "    for mode in modes_list:\n",
        "        method = mode['method']\n",
        "        steps = parse_steps(mode['steps'])\n",
        "        methods.append(CookingMethod(method=method, steps=steps))\n",
        "    return methods\n",
        "\n",
        "# Step 3: Prepare the OpenAI API request\n",
        "# Define the API endpoint, headers, and request data\n",
        "url = 'https://api.openai.com/v1/chat/completions'\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {OPENAI_API_KEY}',\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "meal = input(\"Enter the name of the dish: \")\n",
        "\n",
        "MAX_TOKENS = 1024\n",
        "data = {\n",
        "    'model': 'gpt-4o',\n",
        "    'messages': [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': 'You are a chef assistant. Respond only with valid JSON. Do not include additional commentary, explanations, or extra text.'\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': f\"\"\"\n",
        "            Provide a recipe for {meal} in the following JSON format:\n",
        "            {{\n",
        "                \"title\": \"<Recipe Title>\",\n",
        "                \"ingredients\": [\n",
        "                    {{\"name\": \"<Ingredient Name>\", \"quantity\": \"<Quantity>\", \"unit\": \"<Unit>\"}}\n",
        "                ],\n",
        "                \"cooking_modes\": [\n",
        "                    {{\n",
        "                        \"method\": \"<Cooking Method>\",\n",
        "                        \"steps\": [\n",
        "                            {{\"step\": <Step Number>, \"description\": \"<Step Description>\"}}\n",
        "                        ]\n",
        "                    }}\n",
        "                ]\n",
        "            }}\n",
        "            \"\"\"\n",
        "        }\n",
        "    ],\n",
        "    'max_tokens': MAX_TOKENS\n",
        "}\n",
        "\n",
        "# Step 4: Send the request and handle the response\n",
        "response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    try:\n",
        "        recipe_text = response.json()['choices'][0]['message']['content'].strip()\n",
        "\n",
        "        # Clean and parse the JSON response\n",
        "        if recipe_text.startswith(\"{\") and recipe_text.endswith(\"}\"):\n",
        "            recipe_data = json.loads(recipe_text)\n",
        "        else:\n",
        "            json_start = recipe_text.find(\"{\")\n",
        "            json_end = recipe_text.rfind(\"}\") + 1\n",
        "            recipe_data = json.loads(recipe_text[json_start:json_end])\n",
        "\n",
        "        # Step 5: Parse structured data\n",
        "        ingredients = parse_ingredients(recipe_data['ingredients'])\n",
        "        cooking_modes = parse_cooking_modes(recipe_data['cooking_modes'])\n",
        "        recipe = Recipe(title=recipe_data['title'], ingredients=ingredients, cooking_modes=cooking_modes)\n",
        "\n",
        "        # Step 6: Convert structured data to DataFrames\n",
        "        ingredients_df = pd.DataFrame([{\n",
        "            'Name': ing.name, 'Quantity': ing.quantity, 'Unit': ing.unit\n",
        "        } for ing in recipe.ingredients])\n",
        "\n",
        "        cooking_modes_data = []\n",
        "        for mode in recipe.cooking_modes:\n",
        "            for step in mode.steps:\n",
        "                cooking_modes_data.append({\n",
        "                    'Method': mode.method,\n",
        "                    'Step': step.step,\n",
        "                    'Description': step.description\n",
        "                })\n",
        "        cooking_modes_df = pd.DataFrame(cooking_modes_data)\n",
        "\n",
        "        # Step 7: Display the results\n",
        "        print(\"\\nIngredients DataFrame:\")\n",
        "        display(ingredients_df)\n",
        "\n",
        "        print(\"\\nCooking Modes DataFrame:\")\n",
        "        display(cooking_modes_df)\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(\"Error: The response could not be parsed as JSON. Please check the output format.\")\n",
        "        print(\"Parsing Error Details:\", e)\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n"
      ],
      "metadata": {
        "id": "KJ-ehi8q2U7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FI0UpaZ87ViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLVpLh1t87po"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}